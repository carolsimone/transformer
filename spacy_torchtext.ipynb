{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Spacy and torchtext\n",
    "<a href=\"https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95\">\n",
    "Really good article</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europarl-v7.fr-en.en europarl-v7.fr-en.fr\r\n"
     ]
    }
   ],
   "source": [
    "path = '../data/nlp/transformer/fr-en'\n",
    "!ls {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "europarl_en = open(f'{path}/europarl-v7.fr-en.en', encoding='utf-8').read().split('\\n')\n",
    "europarl_fr = open(f'{path}/europarl-v7.fr-en.fr', encoding='utf-8').read().split('\\n')\n",
    "\n",
    "#!python3 -m spacy download en\n",
    "#!python3 -m spacy download fr\n",
    "#!pip install torchtext==0.6.0\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "en = spacy.load('en')\n",
    "fr = spacy.load('fr')\n",
    "\n",
    "def tokenize_en(sentence):\n",
    "    return [tok.text for tok in en.tokenizer(sentence)]\n",
    "\n",
    "def tokenize_fr(sentence):\n",
    "    return [tok.text for tok in fr.tokenizer(sentence)]\n",
    "\n",
    "# Before you create the field, and at then end you build the vocab from these two Fields\n",
    "EN_TEXT = Field(tokenize=tokenize_en)\n",
    "FR_TEXT = Field(tokenize=tokenize_fr, init_token='<sos>', eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps counter-intuitively, the best way to work with Torchtext is to turn your data into spreadsheet format, no matter the original format of your data file.\n",
    "\n",
    "This is due to the incredible versatility of the Torchtext TabularDataset function, which creates datasets from spreadsheet formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# csv format\n",
    "raw_data = {'English' : [line for line in europarl_en], 'French': [line for line in europarl_fr]}\n",
    "\n",
    "df = pd.DataFrame(raw_data, columns=[\"English\", \"French\"])\n",
    "\n",
    "# remove very long sentences and sentences where translations are \n",
    "# not of roughly equal length\n",
    "df['eng_len'] = df['English'].str.count(' ')\n",
    "df['fr_len'] = df['French'].str.count(' ')\n",
    "df = df.query('fr_len < 80 & eng_len < 80')\n",
    "df = df.query('fr_len < eng_len * 1.5 & fr_len * 1.5 > eng_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>eng_len</th>\n",
       "      <th>fr_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "      <td>Reprise de la session</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "      <td>Je déclare reprise la session du Parlement eur...</td>\n",
       "      <td>37</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded '...</td>\n",
       "      <td>Comme vous avez pu le constater, le grand \"bog...</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in...</td>\n",
       "      <td>Vous avez souhaité un débat à ce sujet dans le...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a mi...</td>\n",
       "      <td>En attendant, je souhaiterais, comme un certai...</td>\n",
       "      <td>39</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0                          Resumption of the session   \n",
       "1  I declare resumed the session of the European ...   \n",
       "2  Although, as you will have seen, the dreaded '...   \n",
       "3  You have requested a debate on this subject in...   \n",
       "4  In the meantime, I should like to observe a mi...   \n",
       "\n",
       "                                              French  eng_len  fr_len  \n",
       "0                              Reprise de la session        3       3  \n",
       "1  Je déclare reprise la session du Parlement eur...       37      32  \n",
       "2  Comme vous avez pu le constater, le grand \"bog...       30      36  \n",
       "3  Vous avez souhaité un débat à ce sujet dans le...       18      18  \n",
       "4  En attendant, je souhaiterais, comme un certai...       39      37  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create train and validation set\n",
    "train, val = train_test_split(df, test_size=0.1)\n",
    "\n",
    "train.to_csv('train.csv', index=False)\n",
    "val.to_csv('val.csv', index=False)\n",
    "\n",
    "data_fields = [('English', EN_TEXT), ('French', FR_TEXT)]\n",
    "train, val = TabularDataset.splits(path='./', train='train.csv', validation='val.csv', \n",
    "                                   format='csv', fields=data_fields)\n",
    "\n",
    "FR_TEXT.build_vocab(train, val)\n",
    "EN_TEXT.build_vocab(train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(EN_TEXT.vocab.stoi['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = BucketIterator(train, batch_size=5, \n",
    "                            sort_key=lambda x: len(x.French), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  156, 44910,   347,  2321,    27],\n",
      "        [   11, 21399,     2,   104, 11051],\n",
      "        [ 1619,     7,    78,   655,   125],\n",
      "        [ 1495,    11,   423,    63,     2],\n",
      "        [ 4303,   303,     3,    35,  4322],\n",
      "        [    3,     5,   470,   519,  1957],\n",
      "        [   13, 10597,   654,    21,     5],\n",
      "        [   21,   159,    24,    46,   391],\n",
      "        [  325,   295,    11,    11,     7],\n",
      "        [  125,   735,  2463,  2672,   147],\n",
      "        [    2,     2,  1397,  2002,   108],\n",
      "        [ 3068,  3041,     3,  3518,  5682],\n",
      "        [   76,   189,   371,     4,    29],\n",
      "        [ 8537,    29,    18,     1,     2],\n",
      "        [    7,  1441,  8661,     1,  3222],\n",
      "        [ 8166, 19753,  1540,     1,   988],\n",
      "        [   18,     3,     7,     1,     4],\n",
      "        [10817,    25,  3490,     1,     1],\n",
      "        [   73,   224, 17539,     1,     1],\n",
      "        [   25,    19,   116,     1,     1],\n",
      "        [ 1370,  1440,   329,     1,     1],\n",
      "        [    6,    11,    10,     1,     1],\n",
      "        [  459,   242,    11,     1,     1],\n",
      "        [   53,   410,    87,     1,     1],\n",
      "        [   58,     3,     7,     1,     1],\n",
      "        [   51,    45,   485,     1,     1],\n",
      "        [  292,    72,  9526,     1,     1],\n",
      "        [    8,    37,  2462,     1,     1],\n",
      "        [    2,  1268,     4,     1,     1],\n",
      "        [  209,     9,     1,     1,     1],\n",
      "        [    5, 19753,     1,     1,     1],\n",
      "        [  955,    20,     1,     1,     1],\n",
      "        [   89,    11,     1,     1,     1],\n",
      "        [    4, 52689,     1,     1,     1],\n",
      "        [    1,  1496,     1,     1,     1],\n",
      "        [    1,     4,     1,     1,     1]])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(batch.English)  # index are words, column is a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silent\n"
     ]
    }
   ],
   "source": [
    "print(EN_TEXT.vocab.itos[44910])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an hack to make the process faster, indeed it is fairly slow. Check the article for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
