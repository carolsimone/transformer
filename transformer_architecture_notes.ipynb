{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Transformer\n",
    "\n",
    "Article : <a href=\"https://www.lesswrong.com/s/nMGrhBYXWjPhZoyNL/p/McmHduRWJynsjZjx5\">lesswrong.com</a>\n",
    "\n",
    "\n",
    "# _Position Encoder_\n",
    "The _Positional Encoder_ is an element wise addition to the _word embedding_ referring to the word in the sentence. For this element wise addition to work, the _positional encoder_ vector must be of the same length of the _word emebedding_ vector, otherwise the code will throw you an error.\n",
    "\n",
    "The positional encoder is calculated using two different formula, and you iterate till `i = d_model / 2`, after that you have created the _positional encoding_ vector for all the positions. The 2 formulas are as follow:\n",
    "\n",
    "$PE_{(pos, 2i)} = sin(pos / 10000^{2i/d_{model}})$\n",
    "\n",
    "$PE_{(pos, 2i+i)} = cos(pos / 10000^{2i/d_{model}})$\n",
    "\n",
    "where `pos` is the position of the word in the sentence, and `i` is the index into the embedding dimension. If model `d_model=512`, then `i` range from `0` to `256` in order to have a complete _positonal encoding vector_.\n",
    "\n",
    "\n",
    "What this means in practice?\n",
    "\n",
    "For example, if we have a word $w$ at $pos \\in [0, L-1]$, where $L$ is the length of the sentence (or the matrix length of the sentences). This means we need to create a _positional encoder_ for all the $w$ between 0 to $L-1$.\n",
    "\n",
    "If we have a 4 dimensional embedding $e_w$ meaning $d_{model} = 4$, and we want to calculate the _positional encoder_ for `pos = 1`, then:\n",
    "\n",
    "$$PE_{1, 0} = sin(\\frac {1} {10000^{2i/d_{model}}}) = sin(\\frac {1} {10000^{2*0/4}}) = sin(\\frac {1} {10000^0})$$\n",
    "\n",
    "$$PE_{1, 0} = cos(\\frac {1} {10000^{2i/d_{model}}}) = cos(\\frac {1} {10000^{2*0/4}}) = cos(\\frac {1} {10000^{0}}) $$\n",
    "\n",
    "$$PE_{1, 1} = sin(\\frac {1} {10000^{2i/d_{model}}}) = sin(\\frac {1} {10000^{1*2/4}}) = sin(\\frac {1} {10000^{1/2}}) $$\n",
    "\n",
    "$$PE_{1, 1} = cos(\\frac {1} {10000^{2i/d_{model}}}) = cos(\\frac {1} {10000^{1*2/4}}) = cos(\\frac {1} {10000^{1/2}}) $$\n",
    "\n",
    "_NOTE:_\n",
    "\n",
    "The second $PE_{1, 0}$ creates the embedding index = 1, the second $PE_{1, 1}$ creates the index = 3, and so on. __This is the only tricky part of it__.\n",
    "\n",
    "\n",
    "# _Self-attention_\n",
    "\n",
    "Create a graph depicting self-attention.\n",
    "\n",
    "# _The stack_\n",
    "Important to note that the number of words inputted from the eoncoder stack are also outputted from the encoder stack. This means that the vector input shape we passed in is also the exact same shape in which the vectors will be outputted from the encoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
